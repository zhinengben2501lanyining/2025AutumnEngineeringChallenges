"""
GPUæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯Dockerå®¹å™¨ä¸­çš„GPUåŠ é€ŸåŠŸèƒ½
"""

import torch
import subprocess
import sys
import os

def check_nvidia_smi():
    """æ£€æŸ¥nvidia-smiå‘½ä»¤æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("=" * 50)
    print("1. æ£€æŸ¥nvidia-smiå‘½ä»¤è¾“å‡º")
    print("=" * 50)
    
    try:
        result = subprocess.run(['nvidia-smi'], 
                              capture_output=True, 
                              text=True, 
                              timeout=10)
        
        if result.returncode == 0:
            print("âœ“ nvidia-smiå‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
            print("\nGPUä¿¡æ¯:")
            print(result.stdout)
            return True
        else:
            print("âœ— nvidia-smiå‘½ä»¤æ‰§è¡Œå¤±è´¥")
            print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âœ— æ‰§è¡Œnvidia-smiæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

def check_pytorch_gpu():
    """æ£€æŸ¥PyTorchæ˜¯å¦èƒ½æ£€æµ‹åˆ°GPU"""
    print("=" * 50)
    print("2. æ£€æŸ¥PyTorch GPUæ”¯æŒ")
    print("=" * 50)
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    cuda_available = torch.cuda.is_available()
    print(f"CUDAå¯ç”¨: {'âœ“' if cuda_available else 'âœ—'}")
    
    if not cuda_available:
        print("CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…")
        return False
    
    # è·å–GPUæ•°é‡
    gpu_count = torch.cuda.device_count()
    print(f"GPUæ•°é‡: {gpu_count}")
    
    # æ˜¾ç¤ºæ¯ä¸ªGPUçš„è¯¦ç»†ä¿¡æ¯
    for i in range(gpu_count):
        print(f"\nGPU {i} è¯¦ç»†ä¿¡æ¯:")
        print(f"  è®¾å¤‡åç§°: {torch.cuda.get_device_name(i)}")
        print(f"  CUDAè®¡ç®—èƒ½åŠ›: sm_{torch.cuda.get_device_capability(i)[0]}{torch.cuda.get_device_capability(i)[1]}")
        print(f"  æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
    
    print(f"\n3. æ‰§è¡Œå…¼å®¹æ€§æµ‹è¯•")
    print("=" * 50)
    
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡æ¥æ•è·æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
        
        # æµ‹è¯•1: ç®€å•çš„å¼ é‡åˆ›å»ºå’Œç§»åŠ¨
        print("æµ‹è¯•1: å¼ é‡åˆ›å»ºå’Œç§»åŠ¨...")
        x = torch.tensor([1.0, 2.0, 3.0]).cuda()
        print(f"âœ“ å¼ é‡åˆ›å»ºæˆåŠŸ: {x.device}")
        
        # æµ‹è¯•2: ç®€å•çš„è®¡ç®—
        print("æµ‹è¯•2: ç®€å•è®¡ç®—...")
        y = x * 2
        print(f"âœ“ è®¡ç®—æˆåŠŸ: {y.cpu().numpy()}")
        
        # æµ‹è¯•3: çŸ©é˜µè¿ç®—ï¼ˆå°è§„æ¨¡ï¼‰
        print("æµ‹è¯•3: å°è§„æ¨¡çŸ©é˜µè¿ç®—...")
        a = torch.randn(100, 100).cuda()
        b = torch.randn(100, 100).cuda()
        c = torch.matmul(a, b)
        print(f"âœ“ çŸ©é˜µè¿ç®—æˆåŠŸ: {c.shape}")
        
        # æµ‹è¯•4: æ£€æŸ¥CUDAåŠŸèƒ½
        print("æµ‹è¯•4: CUDAåŠŸèƒ½æ£€æŸ¥...")
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
        print(f"  è®¾å¤‡å±æ€§: {torch.cuda.get_device_properties(0)}")
        
        return True
        
    except Exception as e:
        print(f"âœ— å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        print("\nè¯¦ç»†è¯Šæ–­:")
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda if hasattr(torch.version, 'cuda') else 'N/A'}")
        print(f"è®¡ç®—èƒ½åŠ›: sm_{torch.cuda.get_device_capability(0)[0]}{torch.cuda.get_device_capability(0)[1]}")        
        return False

def check_system_info():
    """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
    print("=" * 50)
    print("ç³»ç»Ÿä¿¡æ¯")
    print("=" * 50)
    
    print(f"Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    if hasattr(torch.version, 'cuda'):
        print(f"PyTorch CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹GPUå…¼å®¹æ€§æµ‹è¯•...")
    print("=" * 60)
    print("ç›®æ ‡è®¾å¤‡: NVIDIA GeForce RTX 5060 (sm_120)")
    print("=" * 60)
    
    # æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯
    check_system_info()
    print()
    
    # æ£€æŸ¥nvidia-smi
    nvidia_smi_success = check_nvidia_smi()
    print()
    
    # æ£€æŸ¥PyTorch GPUæ”¯æŒ
    pytorch_gpu_success = check_pytorch_gpu()
    print()
    
    # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
    print("=" * 60)
    print("æµ‹è¯•ç»“æœæ‘˜è¦:")
    print("=" * 60)
    
    print(f"nvidia-smiæµ‹è¯•: {'é€šè¿‡ âœ“' if nvidia_smi_success else 'å¤±è´¥ âœ—'}")
    print(f"PyTorch GPUæµ‹è¯•: {'é€šè¿‡ âœ“' if pytorch_gpu_success else 'å¤±è´¥ âœ—'}")
    
    if nvidia_smi_success and pytorch_gpu_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GPUåŠ é€Ÿç¯å¢ƒé…ç½®æˆåŠŸï¼")
        print("æ‚¨çš„RTX 5060 GPUç°åœ¨å¯ä»¥æ­£å¸¸ä½¿ç”¨PyTorchè¿›è¡ŒåŠ é€Ÿè®¡ç®—")
        return 0
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥")
        return 1

if __name__ == "__main__":
    sys.exit(main())