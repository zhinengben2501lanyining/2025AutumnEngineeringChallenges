# Level2 å®è·µæ—¥å¿—

æŒ‰ç…§æ–‡æ¡£çš„æ“ä½œæµç¨‹ï¼Œåœ¨å®‰è£…æ­¥éª¤ `é…ç½®NVIDIA Container Toolkit` å¤„å‘ç°å¼‚å¸¸ï¼Œç»æ£€æŸ¥ï¼Œç›¸å…³bashå‘½ä»¤åº”è¯¥æ›¿æ¢ä¸º

```bash
#éªŒè¯é©±åŠ¨çŠ¶æ€
nvidia-smi

#å®‰è£… NVIDIA Container Toolkit
sudo apt-get update
sudo apt-get install -y curl
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

#å®‰è£… NVIDIA Container Toolkit
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

# æ›´æ–° Docker é…ç½®
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

#æµ‹è¯•GPUè®¿é—®

sudo docker run --rm --gpus all nvidia/cuda:12.6.0-base-ubuntu24.04 nvidia-smi
```

è§‚å¯Ÿåˆ°æ–‡æ¡£å¹¶æœªæä¾› `gpu_test.py` ä½¿ç”¨ `DeepSeek` ç”Ÿæˆäº†ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶

```python
"""
GPUæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯Dockerå®¹å™¨ä¸­çš„GPUåŠ é€ŸåŠŸèƒ½
"""

import torch
import subprocess
import sys
import os

def check_nvidia_smi():
    """æ£€æŸ¥nvidia-smiå‘½ä»¤æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("=" * 50)
    print("1. æ£€æŸ¥nvidia-smiå‘½ä»¤è¾“å‡º")
    print("=" * 50)
    
    try:
        result = subprocess.run(['nvidia-smi'], 
                              capture_output=True, 
                              text=True, 
                              timeout=10)
        
        if result.returncode == 0:
            print("âœ“ nvidia-smiå‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
            print("\nGPUä¿¡æ¯:")
            print(result.stdout)
            return True
        else:
            print("âœ— nvidia-smiå‘½ä»¤æ‰§è¡Œå¤±è´¥")
            print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âœ— æ‰§è¡Œnvidia-smiæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

def check_pytorch_gpu():
    """æ£€æŸ¥PyTorchæ˜¯å¦èƒ½æ£€æµ‹åˆ°GPU"""
    print("=" * 50)
    print("2. æ£€æŸ¥PyTorch GPUæ”¯æŒ")
    print("=" * 50)
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    cuda_available = torch.cuda.is_available()
    print(f"CUDAå¯ç”¨: {'âœ“' if cuda_available else 'âœ—'}")
    
    if not cuda_available:
        print("CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…")
        return False
    
    # è·å–GPUæ•°é‡
    gpu_count = torch.cuda.device_count()
    print(f"GPUæ•°é‡: {gpu_count}")
    
    # æ˜¾ç¤ºæ¯ä¸ªGPUçš„è¯¦ç»†ä¿¡æ¯
    for i in range(gpu_count):
        print(f"\nGPU {i} è¯¦ç»†ä¿¡æ¯:")
        print(f"  è®¾å¤‡åç§°: {torch.cuda.get_device_name(i)}")
        print(f"  CUDAè®¡ç®—èƒ½åŠ›: sm_{torch.cuda.get_device_capability(i)[0]}{torch.cuda.get_device_capability(i)[1]}")
        print(f"  æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
    
    print(f"\n3. æ‰§è¡Œå…¼å®¹æ€§æµ‹è¯•")
    print("=" * 50)
    
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡æ¥æ•è·æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
        
        # æµ‹è¯•1: ç®€å•çš„å¼ é‡åˆ›å»ºå’Œç§»åŠ¨
        print("æµ‹è¯•1: å¼ é‡åˆ›å»ºå’Œç§»åŠ¨...")
        x = torch.tensor([1.0, 2.0, 3.0]).cuda()
        print(f"âœ“ å¼ é‡åˆ›å»ºæˆåŠŸ: {x.device}")
        
        # æµ‹è¯•2: ç®€å•çš„è®¡ç®—
        print("æµ‹è¯•2: ç®€å•è®¡ç®—...")
        y = x * 2
        print(f"âœ“ è®¡ç®—æˆåŠŸ: {y.cpu().numpy()}")
        
        # æµ‹è¯•3: çŸ©é˜µè¿ç®—ï¼ˆå°è§„æ¨¡ï¼‰
        print("æµ‹è¯•3: å°è§„æ¨¡çŸ©é˜µè¿ç®—...")
        a = torch.randn(100, 100).cuda()
        b = torch.randn(100, 100).cuda()
        c = torch.matmul(a, b)
        print(f"âœ“ çŸ©é˜µè¿ç®—æˆåŠŸ: {c.shape}")
        
        # æµ‹è¯•4: æ£€æŸ¥CUDAåŠŸèƒ½
        print("æµ‹è¯•4: CUDAåŠŸèƒ½æ£€æŸ¥...")
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
        print(f"  è®¾å¤‡å±æ€§: {torch.cuda.get_device_properties(0)}")
        
        return True
        
    except Exception as e:
        print(f"âœ— å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        print("\nè¯¦ç»†è¯Šæ–­:")
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda if hasattr(torch.version, 'cuda') else 'N/A'}")
        print(f"è®¡ç®—èƒ½åŠ›: sm_{torch.cuda.get_device_capability(0)[0]}{torch.cuda.get_device_capability(0)[1]}")        
        return False

def check_system_info():
    """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
    print("=" * 50)
    print("ç³»ç»Ÿä¿¡æ¯")
    print("=" * 50)
    
    print(f"Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    if hasattr(torch.version, 'cuda'):
        print(f"PyTorch CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹GPUå…¼å®¹æ€§æµ‹è¯•...")
    print("=" * 60)
    print("ç›®æ ‡è®¾å¤‡: NVIDIA GeForce RTX 5060 (sm_120)")
    print("=" * 60)
    
    # æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯
    check_system_info()
    print()
    
    # æ£€æŸ¥nvidia-smi
    nvidia_smi_success = check_nvidia_smi()
    print()
    
    # æ£€æŸ¥PyTorch GPUæ”¯æŒ
    pytorch_gpu_success = check_pytorch_gpu()
    print()
    
    # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
    print("=" * 60)
    print("æµ‹è¯•ç»“æœæ‘˜è¦:")
    print("=" * 60)
    
    print(f"nvidia-smiæµ‹è¯•: {'é€šè¿‡ âœ“' if nvidia_smi_success else 'å¤±è´¥ âœ—'}")
    print(f"PyTorch GPUæµ‹è¯•: {'é€šè¿‡ âœ“' if pytorch_gpu_success else 'å¤±è´¥ âœ—'}")
    
    if nvidia_smi_success and pytorch_gpu_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GPUåŠ é€Ÿç¯å¢ƒé…ç½®æˆåŠŸï¼")
        print("æ‚¨çš„RTX 5060 GPUç°åœ¨å¯ä»¥æ­£å¸¸ä½¿ç”¨PyTorchè¿›è¡ŒåŠ é€Ÿè®¡ç®—")
        return 0
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥")
        return 1

if __name__ == "__main__":
    sys.exit(main())
```

```Dockerfile
FROM nvidia/cuda:12.6.0-base-ubuntu24.04

RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…æ”¯æŒCUDA 12.4å’Œæœ€æ–°æ¶æ„çš„PyTorch
RUN pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu124

# å¤åˆ¶æµ‹è¯•è„šæœ¬
COPY gpu_test.py /app/gpu_test.py

WORKDIR /app

# è®¾ç½®æ‰§è¡Œæƒé™
RUN chmod +x gpu_test.py

CMD ["python3", "gpu_test.py"]
```

æŠ¥é”™ï¼Œå‘ç° `pip` å‘½ä»¤éœ€è¦æ·»åŠ  `--break-system-packages` å‚æ•°(Ubuntu24.02çš„å®‰å…¨æªæ–½)

GPUæµ‹è¯•å¤±è´¥ï¼Œå‘ç° `Pytorch` å¯¹åº”çš„cudaç‰ˆæœ¬è¿‡ä½ï¼Œä¿®æ”¹pipå‘½ä»¤ä¸º `pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu130 --break-system-packages`

# ä»»åŠ¡ç›®æ ‡
1. nvidia-smi å‘½ä»¤åœ¨å®¹å™¨å†…æ­£å¸¸è¿è¡Œ è§ `nvidia-smi.png`
2. PyTorchèƒ½å¤Ÿæ£€æµ‹åˆ°GPUè®¾å¤‡ è§ `pytorch1/2.png`
3. æ˜¾ç¤ºæ­£ç¡®çš„GPUå‹å·å’Œæ˜¾å­˜ä¿¡æ¯ è§ `l2log/nvidia-smi.png` æˆ– `l2log.log`
